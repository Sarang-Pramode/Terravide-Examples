{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Defined Package\n",
    "import src.modules.utils as util\n",
    "import src.modules.MultipleReturnsClassification as MRC\n",
    "import src.modules.LasFilePreprocessing as LFP\n",
    "import src.modules.ExtractGroundPlane as GP\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score # How best can we seperate clusters\n",
    "import pptk\n",
    "import time\n",
    "from scipy import spatial\n",
    "import re #parsing log files\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial import ConvexHull\n",
    "import alphashape\n",
    "from shapely.geometry import Point\n",
    "import json\n",
    "import laspy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging Functions\n",
    "\n",
    "def InitiateLogger(filename: str, year:int)-> None:\n",
    "\n",
    "    LoggerPath = \"Datasets/\"+\"Package_Generated/\"+filename[:-4]+\"/\"+str(year)+\"/Logs_\"+filename[:-4]+\"/\"\n",
    "\n",
    "    print(\"Logger Folder Path : \",LoggerPath)\n",
    "    # Check whether the specified pptk_capture_path exists or not\n",
    "    isExist = os.path.exists(LoggerPath)\n",
    "\n",
    "    if not isExist:\n",
    "    # Create a new directory because it does not exist \n",
    "        os.makedirs(LoggerPath)\n",
    "\n",
    "    logfilename = LoggerPath + filename+'.log' \n",
    "    logger = logging.getLogger()\n",
    "    fhandler = logging.FileHandler(filename=logfilename, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def parse_log_Completedfiles(log_file_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        log_file_path (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \n",
    "    Usage:\n",
    "    log_file_path = 'TEST_log.log'\n",
    "    filenames = parse_log_Completedfiles(log_file_path)\n",
    "    print(filenames)\n",
    "    \"\"\"\n",
    "    filenames = []\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            match = re.search(r'(?<=Completed file: )\\S+', line)\n",
    "            if match:\n",
    "                filenames.append(match.group())\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_filenames(folder_path:str, year:int)->list:\n",
    "    folder_path = folder_path+'NYC_'+str(year)+'/'\n",
    "    filenames = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return filenames\n",
    "\n",
    "def Delete_File(file_path:str)->None:\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_path} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"{file_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Ground\n",
    "def Extract_GroundData(filename:str,year:int, TileDivision=30, lasfiles_folderpath= 'Datasets/FTP_files/LiDAR/') -> tuple:\n",
    "    \n",
    "    s_start = time.time()\n",
    "    rows, cols = (TileDivision, TileDivision)\n",
    "\n",
    "    lasfilepath = lasfiles_folderpath+'NYC_'+str(year)+'/'+filename\n",
    "    lasfile_object = LFP.Read_lasFile(lasfilepath)\n",
    "    lidar_df, rawpoints = LFP.Create_lasFileDataframe(lasfileObject=lasfile_object)\n",
    "\n",
    "    #MR_df = LFP.Get_MRpoints(lidar_df) Not needed for ground points\n",
    "    SR_df = LFP.Get_SRpoints(lidar_df)\n",
    "\n",
    "    #lasTile class\n",
    "    TileObj = LFP.lasTile(SR_df,TileDivision)\n",
    "\n",
    "    #Serialized\n",
    "    s_start = time.time()\n",
    "    lidar_TilesubsetArr = TileObj.Get_subtileArray()\n",
    "\n",
    "    s_end = time.time()\n",
    "    stime = s_end - s_start\n",
    "    logging.info(\"Extraction of Subtile Matrix Buffer Serial Time for %s = %d\",filename,stime)\n",
    "\n",
    "    #Ground Plane Classifcation - Serial Implementation\n",
    "    g_start = time.time()\n",
    "    Potential_Ground_Points = []\n",
    "    Other_points = []\n",
    "\n",
    "    GP_obj = GP.GP_class()\n",
    "\n",
    "    for row in range(TileDivision):\n",
    "        for col in range(TileDivision):\n",
    "\n",
    "            tile_segment_points = lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()\n",
    "\n",
    "            Ground_Points, NGround_Points = GP_obj.Extract_GroundPoints(tile_segment_points)\n",
    "\n",
    "            for k in Ground_Points:\n",
    "                Potential_Ground_Points.append(k) #append points which may be potentially ground points\n",
    "            for l in NGround_Points:\n",
    "                Other_points.append(l) #append points which may be potentially ground points\n",
    "    Potential_Ground_Points = np.array(Potential_Ground_Points)\n",
    "    Other_points = np.array(Other_points)\n",
    "    \n",
    "    g_end = time.time()\n",
    "    gtime = g_end - g_start\n",
    "    logging.info(\"Ground Point Extraction Algorithm Serial Time for %s = %d\",filename,gtime)\n",
    "\n",
    "    return Potential_Ground_Points, Other_points, np.mean(Potential_Ground_Points[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MR Classification\n",
    "def Log_TileLocation(MR_df):\n",
    "\n",
    "    #Print Lat , Long\n",
    "    ix, iy = np.mean(MR_df.X.to_numpy()), np.mean(MR_df.Y.to_numpy()) \n",
    "\n",
    "    transformer = Transformer.from_crs(\"epsg:2263\", \"epsg:4326\")\n",
    "    lat, lon = transformer.transform(ix*3.28, iy*3.28)\n",
    "    location_str = str(lat)+\",\"+str(lon)\n",
    "\n",
    "    return location_str, lat,lon\n",
    "\n",
    "def ConvertLatLong(points): # converting to ft ( 2263 - 4326)\n",
    "\n",
    "    x = points[0]*3.28\n",
    "    y = points[1]*3.28\n",
    "    transformer = Transformer.from_crs(\"epsg:2263\", \"epsg:4326\")\n",
    "    lat, lon = transformer.transform(x,y)\n",
    "    return [lat,lon]\n",
    "\n",
    "def PreprocessLasFile(f, year, TileDivision=10, lasfiles_folderpath='Datasets/FTP_files/LiDAR/'):\n",
    "\n",
    "    #Get File_ID -> 1001.las - 1001\n",
    "    las_fileID = int(''.join(c for c in f if c.isdigit())) \n",
    "    #Object to handle las preprocessing\n",
    "    LasHandling = MRC.LFP\n",
    "    #Get path\n",
    "    lasfilepath = lasfiles_folderpath+'NYC_'+str(year)+'/'+str(las_fileID)+'.las'\n",
    "    #Read las file\n",
    "    lasfile_object = LasHandling.Read_lasFile(lasfilepath)\n",
    "    #Define Tile Subdivisions\n",
    "    TileDivision = TileDivision\n",
    "    #rows, cols = (TileDivision, TileDivision)\n",
    "    #Create Dataframe from lasfile\n",
    "    lidar_df, rawpoints = LasHandling.Create_lasFileDataframe(lasfileObject=lasfile_object)\n",
    "\n",
    "    #Extract MR and SR points from Dataframe\n",
    "    MR_df = LasHandling.Get_MRpoints(lidar_df)\n",
    "    SR_df = LasHandling.Get_SRpoints(lidar_df)\n",
    "\n",
    "    return lidar_df, rawpoints, MR_df, SR_df\n",
    "\n",
    "def Get_eps_NN_KneeMethod(cluster_df, N_neighbors = 12, display_plot=False):\n",
    "\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=N_neighbors)\n",
    "    neighbors = nearest_neighbors.fit(cluster_df)\n",
    "    distances, indices = neighbors.kneighbors(cluster_df)\n",
    "    distances = np.sort(distances[:,N_neighbors-1], axis=0)\n",
    "\n",
    "    i = np.arange(len(distances))\n",
    "    knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "    if (display_plot):\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        knee.plot_knee()\n",
    "        plt.xlabel(\"Points\")\n",
    "        plt.ylabel(\"Distance\")\n",
    "        print(distances[knee.knee])\n",
    "    \n",
    "    return distances[knee.knee]\n",
    "\n",
    "def Normalize_points(points):\n",
    "    return points / np.linalg.norm(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileCount : 12 .las files found in path = Datasets/FTP_files/LiDAR/\n",
      "25192.las\n"
     ]
    }
   ],
   "source": [
    "#MAIN\n",
    "stime = time.time()\n",
    "\n",
    "#Get las filenames\n",
    "lasfiles_folderpath = 'Datasets/FTP_files/LiDAR/'\n",
    "year = 2021\n",
    "\n",
    "LAS_filenames = Get_filenames(lasfiles_folderpath, year)\n",
    "\n",
    "print(\"FileCount : \"+str(len(LAS_filenames))+\" .las files found in path = \"+lasfiles_folderpath )\n",
    "\n",
    "#NOTE: Script assumes that all files have been downloaded\n",
    "f = LAS_filenames[3]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasfilepath = lasfiles_folderpath+'NYC_'+str(year)+'/'+f\n",
    "lasfile_object = LFP.Read_lasFile(lasfilepath)\n",
    "lidar_df, rawpoints = LFP.Create_lasFileDataframe(lasfileObject=lasfile_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    7407337\n",
       "2.0    3542877\n",
       "1.0    2765796\n",
       "6.0    2353862\n",
       "4.0     673315\n",
       "8.0     397579\n",
       "3.0     242659\n",
       "Name: classification, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_df.classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_df = lidar_df[lidar_df[\"classification\"] != 18] #removing high noise\n",
    "lidar_df = lidar_df[lidar_df[\"classification\"] != 7] #removing  noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40.69844933830165, -73.8486019337876]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvertLatLong([np.mean(lidar_df.X/3.28), np.mean(lidar_df.Y/3.28)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ground = lidar_df[lidar_df[\"classification\"] == 2]\n",
    "Buildings = lidar_df[lidar_df[\"classification\"] == 6]\n",
    "\n",
    "#Extract classification labels 3 4 and 5\n",
    "Vegetation = lidar_df[lidar_df[\"classification\"] == 3]\n",
    "Vegetation = Vegetation.append(lidar_df[lidar_df[\"classification\"] == 4])\n",
    "Vegetation = Vegetation.append(lidar_df[lidar_df[\"classification\"] == 5])\n",
    "\n",
    "Ground = Ground.iloc[:,:3].to_numpy() #Get only X,Y,Z\n",
    "Buildings = Buildings.iloc[:,:3].to_numpy() #Get only X,Y,Z\n",
    "Vegetation = Vegetation.iloc[:,:3].to_numpy() #Get only X,Y,Z\n",
    "\n",
    "All_points_1 = np.concatenate((Ground, Buildings, Vegetation), axis=0)\n",
    "rgb_Ground =  [[1,0,0]]*len(Ground) #Set red colour\n",
    "rgb_Buildings = [[255,255,255]]*len(Buildings) #set green colour - Classified tree points\n",
    "rbg_Vegetation = [[0,1,0]]*len(Vegetation) #set blue colour - Classified Vegetation points\n",
    "All_rgb = np.concatenate((rgb_Ground, rgb_Buildings, rbg_Vegetation), axis=0)\n",
    "\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger Folder Path :  Datasets/Package_Generated/25192/2021/Logs_25192/\n"
     ]
    }
   ],
   "source": [
    "#Initate logger\n",
    "InitiateLogger(f,year)\n",
    "logging.info(\"TerraVide lidar processing Initated\")\n",
    "\n",
    "logging.info(\"Ground Plane Algorithm initated for : \"+f)\n",
    "if year == 2021:\n",
    "    Gpoints = lidar_df[lidar_df[\"classification\"] == 2].iloc[:,:3].to_numpy()\n",
    "    NGpoints = lidar_df[lidar_df[\"classification\"] != 2].iloc[:,:3].to_numpy()\n",
    "    Elevation = np.mean(Gpoints[:,2])\n",
    "else:\n",
    "    Gpoints, NGpoints, Elevation = Extract_GroundData(f, year, TileDivision=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.02591137e+06, 1.93770615e+05, 2.82286585e+01],\n",
       "       [1.02591303e+06, 1.93769380e+05, 2.87231707e+01],\n",
       "       [1.02591343e+06, 1.93769682e+05, 2.86024390e+01],\n",
       "       ...,\n",
       "       [1.02660230e+06, 1.93731573e+05, 2.88128049e+01],\n",
       "       [1.02662911e+06, 1.93743216e+05, 2.95707317e+01],\n",
       "       [1.02660625e+06, 1.93733632e+05, 3.11646341e+01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vegetation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing 2021 25192.las\n",
    "\n",
    "#Extract classification labels 3 4 and 5\n",
    "Vegetation = lidar_df[lidar_df[\"classification\"] == 3]\n",
    "Vegetation = Vegetation.append(lidar_df[lidar_df[\"classification\"] == 4])\n",
    "Vegetation = Vegetation.append(lidar_df[lidar_df[\"classification\"] == 5])\n",
    "\n",
    "Vegetation_df = Vegetation.iloc[:,:3]\n",
    "\n",
    "TileObj_MR = MRC.MR_class(Vegetation_df,TileDivision=10) #Multiple Return Points\n",
    "\n",
    "#Serialized Creation of Lidar Subtiles\n",
    "lidar_TilesubsetArr = TileObj_MR.Get_subtileArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trees buffer\n",
    "\n",
    "Tilecounter = 0\n",
    "Trees_Buffer = []\n",
    "N_Neighbours = 12\n",
    "\n",
    "for row in range(10):\n",
    "    for col in range(10):\n",
    "\n",
    "        #print('-'*40)\n",
    "        \n",
    "        #print(\"TILE ID : \",Tilecounter)\n",
    "        Tilecounter = Tilecounter + 1\n",
    "\n",
    "        if (len(lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()) > N_Neighbours):\n",
    "\n",
    "            cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "            tile_eps = Get_eps_NN_KneeMethod(cluster_df) #round(Optimal_EPS,2)\n",
    "            #print(tile_eps)\n",
    "            tile_segment_points = lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()\n",
    "            subTileTree_Points,  _ = TileObj_MR.Classify_MultipleReturns(tile_segment_points,tile_eps)\n",
    "\n",
    "            for t in subTileTree_Points:\n",
    "                Trees_Buffer.append(t)\n",
    "            \n",
    "            logging.info(\"MR - T_ID : %s - ACTION: Trees Added to - S_ID : %d\",f,Tilecounter)\n",
    "        \n",
    "        else:\n",
    "            logging.warn(\"Empty Tileset Found\")\n",
    "\n",
    "Trees_Buffer = np.array(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x7fea0970c240>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pptk.viewer(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approx_locations_str, T_lat, T_lon = Log_TileLocation(Vegetation_df)\n",
    "logging.info(\"Approximate Location of %s : %s\", f,Approx_locations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_eps = [] #Stores all eps values by tile id\n",
    "N_Neighbours = 12\n",
    "subT_ID = 0\n",
    "TileDivision =10\n",
    "EPS_distribution_df = pd.DataFrame(columns=['T_ID', 'T_lat', 'T_lon', 'subT_ID', 'subT_lat','subT_lon','EPS'])\n",
    "\n",
    "for row in range(TileDivision):\n",
    "    for col in range(TileDivision):\n",
    "\n",
    "        if(len(lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()) > N_Neighbours):\n",
    "\n",
    "            cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "            subtile_location_str, subT_lat, subT_long = Log_TileLocation(cluster_df)\n",
    "            subtile_eps = Get_eps_NN_KneeMethod(cluster_df)\n",
    "            All_eps.append(subtile_eps)\n",
    "\n",
    "        EPS_dist_df_row = [f,T_lat,T_lon]\n",
    "        EPS_dist_df_row.append(subT_ID)\n",
    "        EPS_dist_df_row.append(subT_lat)\n",
    "        EPS_dist_df_row.append(subT_long)\n",
    "        EPS_dist_df_row.append(subtile_eps)\n",
    "\n",
    "        EPS_distribution_df.loc[len(EPS_distribution_df.index)] = EPS_dist_df_row\n",
    "        \n",
    "        subT_ID = subT_ID + 1\n",
    "\n",
    "Optimal_EPS = np.mean(All_eps)\n",
    "logging.info(\"Avg EPS for %s : %s\",f,Optimal_EPS)\n",
    "\n",
    "EPS_CSV_filename = 'Spatial_HP_Distribution_'+f+\"_\"+str(year)+'.csv'\n",
    "EPS_CSV_dir = \"Datasets/\"+\"Package_Generated/\"+f[:-4]+\"/\"+str(year)+\"/LiDAR_HP_MATRIX_\"+f[:-4]+\"/\"\n",
    "# Check whether the specified EPS_CSV_dir exists or not\n",
    "isExist = os.path.exists(EPS_CSV_dir)\n",
    "\n",
    "if not isExist:\n",
    "# Create a new directory because it does not exist \n",
    "    os.makedirs(EPS_CSV_dir)\n",
    "\n",
    "logging.info(\"MR - T_ID : %s - ACTION: HP_MATRIX CSV file Created\",f)\n",
    "EPS_distribution_df.to_csv(EPS_CSV_dir+EPS_CSV_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=np.mean(EPS_distribution_df.EPS), min_samples=30).fit(Trees_Buffer)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "DB_labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array = assign_colors(Trees_Buffer, DB_labels)\n",
    "v = pptk.viewer(Trees_Buffer)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "#TakeScreenShot(Trees_Buffer,v,f,year,\"MR_Points\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of processing 2021 25192.las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TakeScreenShot(points,v,f,year,type:str):\n",
    "    x_mean = np.mean(points[:,0])\n",
    "    y_mean = np.mean(points[:,1])\n",
    "    z_mean = np.mean(points[:,2])\n",
    "\n",
    "    v.set(phi=np.pi/4)\n",
    "    v.set(theta=np.pi/6)\n",
    "    v.set(r=900)\n",
    "    v.set(lookat=[x_mean,y_mean,0])\n",
    "\n",
    "    pptk_capture_path = \"Datasets/\"+\"Package_Generated/\"+f[:-4]+\"/\"+str(year)+\"/PPTK_screenshots_\"+f[:-4]+\"/\"\n",
    "    # Check whether the specified pptk_capture_path exists or not\n",
    "    isExist = os.path.exists(pptk_capture_path)\n",
    "\n",
    "    if not isExist:\n",
    "    # Create a new directory because it does not exist \n",
    "        os.makedirs(pptk_capture_path)\n",
    "    time.sleep(5) #screenshots were not captured after v.close was added\n",
    "    v.capture(pptk_capture_path+\"Capture_\"+type+\"_\"+f[:-4]+\"_\"+str(year)+'.png')\n",
    "    time.sleep(3) #screenshots were not captured after v.close was added\n",
    "    v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting ground poitns found\n",
    "p1 = Gpoints\n",
    "p2 = NGpoints\n",
    "All_points_1 = np.concatenate((p1, p2), axis=0)\n",
    "rgb_p1 =  [[1,0,0]]*len(p1) #Set red colour\n",
    "rgb_p2 = [[255,255,255]]*len(p2) #set green colour - Classified tree points\n",
    "All_rgb = np.concatenate((rgb_p1, rgb_p2,), axis=0)\n",
    "\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "#TakeScreenShot(All_points_1,v,f,year,\"Ground_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"MR Classification Algorithm initated for : \"+f)\n",
    "\n",
    "lidar_df, rawpoints, MR_df, SR_df = PreprocessLasFile(f, year)\n",
    "\n",
    "Approx_locations_str, T_lat, T_lon = Log_TileLocation(MR_df)\n",
    "logging.info(\"Approximate Location of %s : %s\", f,Approx_locations_str)\n",
    "\n",
    "#lasTile class\n",
    "TileObj_SR = MRC.MR_class(SR_df,TileDivision=10) #Single Return Points\n",
    "TileObj_MR = MRC.MR_class(MR_df,TileDivision=10) #Multiple Return Points\n",
    "\n",
    "#Serialized Creation of Lidar Subtiles\n",
    "lidar_TilesubsetArr = TileObj_MR.Get_subtileArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_eps = [] #Stores all eps values by tile id\n",
    "N_Neighbours = 12\n",
    "subT_ID = 0\n",
    "TileDivision =10\n",
    "EPS_distribution_df = pd.DataFrame(columns=['T_ID', 'T_lat', 'T_lon', 'subT_ID', 'subT_lat','subT_lon','EPS'])\n",
    "\n",
    "for row in range(TileDivision):\n",
    "    for col in range(TileDivision):\n",
    "\n",
    "        if(len(lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()) > N_Neighbours):\n",
    "\n",
    "            cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "            subtile_location_str, subT_lat, subT_long = Log_TileLocation(cluster_df)\n",
    "            subtile_eps = Get_eps_NN_KneeMethod(cluster_df)\n",
    "            All_eps.append(subtile_eps)\n",
    "\n",
    "        EPS_dist_df_row = [f,T_lat,T_lon]\n",
    "        EPS_dist_df_row.append(subT_ID)\n",
    "        EPS_dist_df_row.append(subT_lat)\n",
    "        EPS_dist_df_row.append(subT_long)\n",
    "        EPS_dist_df_row.append(subtile_eps)\n",
    "\n",
    "        EPS_distribution_df.loc[len(EPS_distribution_df.index)] = EPS_dist_df_row\n",
    "        \n",
    "        subT_ID = subT_ID + 1\n",
    "\n",
    "Optimal_EPS = np.mean(All_eps)\n",
    "logging.info(\"Avg EPS for %s : %s\",f,Optimal_EPS)\n",
    "\n",
    "EPS_CSV_filename = 'Spatial_HP_Distribution_'+f+\"_\"+str(year)+'.csv'\n",
    "EPS_CSV_dir = \"Datasets/\"+\"Package_Generated/\"+f[:-4]+\"/\"+str(year)+\"/LiDAR_HP_MATRIX_\"+f[:-4]+\"/\"\n",
    "# Check whether the specified EPS_CSV_dir exists or not\n",
    "isExist = os.path.exists(EPS_CSV_dir)\n",
    "\n",
    "if not isExist:\n",
    "# Create a new directory because it does not exist \n",
    "    os.makedirs(EPS_CSV_dir)\n",
    "\n",
    "logging.info(\"MR - T_ID : %s - ACTION: HP_MATRIX CSV file Created\",f)\n",
    "EPS_distribution_df.to_csv(EPS_CSV_dir+EPS_CSV_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tilecounter = 0\n",
    "Trees_Buffer = []\n",
    "N_Neighbours = 12\n",
    "\n",
    "for row in range(TileDivision):\n",
    "    for col in range(TileDivision):\n",
    "\n",
    "        #print('-'*40)\n",
    "        \n",
    "        #print(\"TILE ID : \",Tilecounter)\n",
    "        Tilecounter = Tilecounter + 1\n",
    "\n",
    "        if (len(lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()) > N_Neighbours):\n",
    "\n",
    "            cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "            tile_eps = Get_eps_NN_KneeMethod(cluster_df) #round(Optimal_EPS,2)\n",
    "            #print(tile_eps)\n",
    "            tile_segment_points = lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()\n",
    "            subTileTree_Points,  _ = TileObj_MR.Classify_MultipleReturns(tile_segment_points,tile_eps)\n",
    "\n",
    "            for t in subTileTree_Points:\n",
    "                Trees_Buffer.append(t)\n",
    "            \n",
    "            logging.info(\"MR - T_ID : %s - ACTION: Trees Added to - S_ID : %d\",f,Tilecounter)\n",
    "        \n",
    "        else:\n",
    "            logging.warn(\"Empty Tileset Found\")\n",
    "\n",
    "Trees_Buffer = np.array(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_colors(arr, clusters):\n",
    "    \"\"\"\n",
    "    Assigns the same color to values in the same cluster in a 3D NumPy array\n",
    "    :param arr: 3D numpy array\n",
    "    :param clusters: list indicating which cluster each value belongs to\n",
    "    :return: 3D numpy array with values in the same cluster assigned the same color\n",
    "    \"\"\"\n",
    "    # Create a set of unique cluster labels\n",
    "    labels = set(clusters)\n",
    "    # Create a dictionary to map cluster labels to colors\n",
    "    color_map = {label: np.random.randint(0, 255, 3) for label in labels}\n",
    "    # Create an empty 3D array with the same shape as the input array\n",
    "    color_arr = np.empty_like(arr)\n",
    "    # Iterate over the input array and assign colors to the corresponding clusters\n",
    "    for i, label in enumerate(clusters):\n",
    "        color_arr[i] = color_map[label]\n",
    "    return color_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=np.mean(EPS_distribution_df.EPS), min_samples=30).fit(Trees_Buffer)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "DB_labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array = assign_colors(Trees_Buffer, DB_labels)\n",
    "v = pptk.viewer(Trees_Buffer)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "TakeScreenShot(Trees_Buffer,v,f,year,\"MR_Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_TreesDf = pd.DataFrame(Trees_Buffer, columns=[\"X\",\"Y\",\"Z\"])\n",
    "MR_TreesDf[\"Cluster_Labels\"] = DB_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "Parks clusters -> clusters with > 2 std from mean points\n",
    "large clusters -> [0.5 std , 1.5 std]\n",
    "small clusters -> clusters with <  2 std from mean points\n",
    "Acceptable Clusters -> [-1.5 std to + 0.5 std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cluster_sizes_within_std(cluster_sizes, std_range=1):\n",
    "#     mean = np.mean(cluster_sizes)\n",
    "#     std = np.std(cluster_sizes)\n",
    "#     lower_bound = mean - (std * std_range)\n",
    "#     upper_bound = mean + (std * std_range)\n",
    "#     return np.where((lower_bound <= cluster_sizes) & (cluster_sizes <= upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = np.bincount(DB_labels[DB_labels != -1]) #exclude outliers\n",
    "C_mean = np.mean(cluster_sizes)\n",
    "C_std = np.std(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"MR - T_ID : %s - Stats : ClusterSize_mean: %s , ClusterSize_std : %s\",f,C_mean,C_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 535, 1104, 1332, 1448, 5246, 6923, 7821])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Largest clusters\n",
    "#Parks clusters -> clusters with > 2 std from mean points\n",
    "Park_Clusters = np.where(cluster_sizes > C_mean + 2*C_std)[0]\n",
    "Filtered_cluster_sizes = np.where(cluster_sizes < C_mean + 2*C_std)[0]\n",
    "FC_mean = np.mean(Filtered_cluster_sizes)\n",
    "FC_std = np.std(Filtered_cluster_sizes)\n",
    "Park_Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4687.396391587488, 2705.73100667339)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC_mean, FC_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"MR - T_ID : %s - STATS [PARK REMOVED]: ClusterSize_mean: %s , ClusterSize_std : %s\",f,FC_mean,FC_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculating thresholds as park cluster sizes make the std >> mean \n",
    "# and hence no clusters were showing up when mean - 1*std was taken\n",
    "\n",
    "#small clusters -> clusters with <  1 std from mean points\n",
    "Small_Clusters = np.where(cluster_sizes <= FC_mean + -1*FC_std)[0]\n",
    "#large clusters -> [0.05 std , 2 std]\n",
    "Large_Clusters = np.where((cluster_sizes > FC_mean + 0.1*FC_std)\n",
    "                        &\n",
    "                        (cluster_sizes <= FC_mean + 3*FC_std))[0]\n",
    "#Acceptable Clusters -> [-1 std to + 0.05 std]  \n",
    "Accepted_clusters = np.where((cluster_sizes > FC_mean + -1*FC_std)\n",
    "                        &\n",
    "                        (cluster_sizes <= FC_mean + 0.5*FC_std))[0] \n",
    "\n",
    "# accepted_clusters = np.where(cluster_sizes < np.mean(cluster_sizes) + 2*np.std(cluster_sizes))[0]\n",
    "# large_clusters = np.where(cluster_sizes > np.mean(cluster_sizes) + 2*np.std(cluster_sizes))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_ClusterPoints(df, label_arr):\n",
    "    temp_df = df[df[\"Cluster_Labels\"].isin(label_arr)]\n",
    "    return temp_df.iloc[:,:3].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Park_CLusterPoints = Get_ClusterPoints(MR_TreesDf, Park_Clusters)\n",
    "Small_ClustersPoints = Get_ClusterPoints(MR_TreesDf, Small_Clusters)\n",
    "Large_ClustersPoints = Get_ClusterPoints(MR_TreesDf, Large_Clusters)\n",
    "Accepted_clustersPoints = Get_ClusterPoints(MR_TreesDf, Accepted_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pptk.viewer.viewer.viewer at 0x7fea097139e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pptk.viewer(Large_ClustersPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Large_Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accepted_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Accepted_clustersPoints\n",
    "p2 = Large_ClustersPoints #Park_CLusterPoints #Accepted_clustersPoints\n",
    "All_points_1 = np.concatenate((p1, p2), axis=0)\n",
    "rgb_p1 = [[1,0,0]]*len(p1) #set red colour\n",
    "rgb_p2 =  [[255,255,255]]*len(p2) #Set white colour \n",
    "All_rgb = np.concatenate((rgb_p1, rgb_p2), axis=0)\n",
    "\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "\n",
    "TakeScreenShot(All_points_1,v,f,year,\"ClusterSegmentation_(WL_RA)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Park Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Clusters from Local Maximas\n",
    "PC_labels = []\n",
    "peaks, _ = find_peaks(Park_CLusterPoints[:,2], distance=500,prominence=3)\n",
    "# Get the x, y, and z coordinates of the local maximas\n",
    "local_maximas = Park_CLusterPoints[peaks]\n",
    "\n",
    "#Perform NN\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(local_maximas)\n",
    "distances, indices = nbrs.kneighbors(Park_CLusterPoints)\n",
    "\n",
    "PC_labels = pd.Series(indices.flatten())\n",
    "\n",
    "rgb_array = assign_colors(Park_CLusterPoints, PC_labels)\n",
    "v = pptk.viewer(Park_CLusterPoints)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "#TakeScreenShot(Park_CLusterPoints,v,f,year,\"Park_Clusters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accepted Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=1.5, min_samples=30).fit(Accepted_clustersPoints)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "AC_labels = db.labels_\n",
    "\n",
    "rgb_array = assign_colors(Accepted_clustersPoints, AC_labels)\n",
    "v = pptk.viewer(Accepted_clustersPoints)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "#TakeScreenShot(Accepted_clustersPoints,v,f,year,\"Regular_TreeClusters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_df = pd.DataFrame(Large_ClustersPoints,columns=[\"X\",\"Y\",\"Z\"])\n",
    "\n",
    "maxSearch = LC_df\n",
    "maxSearch['seeds'] = 1\n",
    "xs = maxSearch['X'].to_numpy()\n",
    "ys = maxSearch['Y'].to_numpy()\n",
    "zs = maxSearch['Z'].to_numpy()\n",
    "seeds = maxSearch['seeds'].to_numpy()\n",
    "\n",
    "neighborhood = 4\n",
    "changes = 1\n",
    "while changes > 0:\n",
    "    changes = 0\n",
    "    i = 0\n",
    "    for x,y,z,seed in zip(xs,ys,zs,seeds):\n",
    "        if seed == 1:\n",
    "            zsearch = zs[xs > x - neighborhood]\n",
    "            xsearch = xs[xs > x - neighborhood]\n",
    "            ysearch = ys[xs > x - neighborhood]\n",
    "            \n",
    "            zsearch = zsearch[xsearch < x + neighborhood]\n",
    "            ysearch = ysearch[xsearch < x + neighborhood]\n",
    "            \n",
    "            zsearch = zsearch[ysearch > y - neighborhood]\n",
    "            ysearch = ysearch[ysearch > y - neighborhood]\n",
    "            \n",
    "            zsearch = zsearch[ysearch < y + neighborhood]\n",
    "\n",
    "            zmax = np.max(zsearch)\n",
    "            if z < zmax:\n",
    "                seeds[i] = 0\n",
    "                changes += 1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        i+=1\n",
    "\n",
    "maxSearch['seeds'] = seeds      \n",
    "localMaxima = maxSearch[maxSearch['seeds']>0]\n",
    "\n",
    "centers=localMaxima[['X','Y','Z']].to_numpy()\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(centers)\n",
    "points = maxSearch[['X','Y','Z']].to_numpy()\n",
    "distances, indices = nbrs.kneighbors(points)\n",
    "maxSearch['nearestNeighbor'] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_labels = pd.Series(indices.flatten())\n",
    "rgb_array = assign_colors(Large_ClustersPoints, LC_labels)\n",
    "v = pptk.viewer(Large_ClustersPoints)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "TakeScreenShot(Accepted_clustersPoints,v,f,year,\"Large_TreeClusters_NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(PC_labels)), len(np.unique(LC_labels)),  len(np.unique(AC_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"MR - T_ID : %s - STATS: Park Trees: %d \",f,len(np.unique(PC_labels)))\n",
    "logging.info(\"MR - T_ID : %s - STATS: Large CLuster Trees: %d \",f,len(np.unique(LC_labels)))\n",
    "logging.info(\"MR - T_ID : %s - STATS: Acceptable Trees: %d \",f,len(np.unique(AC_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_TreeCount = len(np.unique(PC_labels))+len(np.unique(LC_labels))+len(np.unique(AC_labels))\n",
    "logging.info(\"MR - T_ID : %s - STATS: TOTAL Trees: %d \",f,Total_TreeCount)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small CLusters\n",
    "\n",
    "#### IGNORED FOR NOW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating JSON File for a single Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_SRpoints(MR_points, NG_df, AreaExtent = 2):\n",
    "\n",
    "    SR_ClusterTreePoints = []\n",
    "    NotTreePoints = []\n",
    "    #create a 2D convex Hull\n",
    "    hull = ConvexHull(MR_points[:,:2])\n",
    "\n",
    "    #Get an area of points to look at (No need to iterate through all points in the tile)\n",
    "    #get bounding subset to look at\n",
    "    SR_shape_X_max = hull.max_bound[0] + AreaExtent\n",
    "    SR_shape_Y_max = hull.max_bound[1] + AreaExtent\n",
    "\n",
    "    SR_shape_X_min = hull.min_bound[0] - AreaExtent\n",
    "    SR_shape_Y_min = hull.min_bound[1] - AreaExtent\n",
    "\n",
    "    lidar_subset_df = NG_df[\n",
    "        (NG_df['X'].between(SR_shape_X_min, SR_shape_X_max, inclusive=False) &\n",
    "    NG_df['Y'].between(SR_shape_Y_min, SR_shape_Y_max, inclusive=False))\n",
    "    ]\n",
    "\n",
    "    #Get only points from BF\n",
    "    SR_masked_points = lidar_subset_df.iloc[:,:3].to_numpy()\n",
    "\n",
    "    #Create a shape of the building footprint\n",
    "    MR_shape_alpha = alphashape.alphashape(MR_points, alpha=0)\n",
    "\n",
    "    for p in SR_masked_points:\n",
    "        tp = Point(p)\n",
    "        if(tp.within(MR_shape_alpha)):\n",
    "            SR_ClusterTreePoints.append(p)\n",
    "        else:\n",
    "            NotTreePoints.append(p)\n",
    "    \n",
    "    return np.array(SR_ClusterTreePoints),np.array(NotTreePoints)\n",
    "\n",
    "def Create_ConvexHullDict(hull):\n",
    "    # Convex Hull Data\n",
    "    ConvexHullData = {\n",
    "        \"vertices\" : hull.vertices.tolist(),\n",
    "        \"simplices\" : hull.simplices.tolist(),\n",
    "        \"ClusterPoints\" : hull.points.tolist(),\n",
    "        \"equations\" : hull.equations.tolist(),\n",
    "        \"volume\" : hull.volume,\n",
    "        \"area\" : hull.area\n",
    "    }\n",
    "\n",
    "    return ConvexHullData\n",
    "\n",
    "def Store_TreeCluster_toJSON(filename, year,\n",
    "                            arr, labels,\n",
    "                            NGpoints_df, \n",
    "                            Extracted_SRpoints, Extracted_NTpoints,\n",
    "                            Global_TreeCounter,\n",
    "                            Elevation,\n",
    "                            jfolderpath, isPark=False):\n",
    "\n",
    "\n",
    "    las_fileID = filename[:-4]\n",
    "\n",
    "    # Get unique labels\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Iterate over unique labels\n",
    "    for label in unique_labels:\n",
    "\n",
    "        if label == -1: #ignore outliers\n",
    "            continue\n",
    "\n",
    "        MR_Cluster_data = arr[labels == label]\n",
    "\n",
    "        if(len(MR_Cluster_data) < 10):\n",
    "            continue\n",
    "\n",
    "        Global_TreeCounter += 1\n",
    "\n",
    "        # Get rows of arr that have the current label\n",
    "        SR_Cluster_data, Extracted_NotTreeData = Get_SRpoints(MR_Cluster_data, NGpoints_df)\n",
    "        \n",
    "        if(len(SR_Cluster_data) == 0):\n",
    "            #No SR points\n",
    "            logging.warn(\"EMPTY SR set found\")\n",
    "            SR_Cluster_data = np.array([]) #empty array\n",
    "            Full_TreePoints = MR_Cluster_data\n",
    "        else:\n",
    "            Full_TreePoints = np.vstack([MR_Cluster_data,SR_Cluster_data])\n",
    "\n",
    "        if(len(SR_Cluster_data)>0):\n",
    "            for sr_p in SR_Cluster_data:\n",
    "                Extracted_SRpoints.append(sr_p)\n",
    "        if(len(Extracted_NotTreeData)>0):\n",
    "            for nt_p in Extracted_NotTreeData:\n",
    "                Extracted_NTpoints.append(nt_p)\n",
    "        \n",
    "        #Generate Convex hull for this cluster\n",
    "        MR_hull = ConvexHull(MR_Cluster_data)\n",
    "        MR_HullInfoDict = Create_ConvexHullDict(MR_hull)\n",
    "\n",
    "        if(len(SR_Cluster_data)> 4):\n",
    "            SR_hull = ConvexHull(SR_Cluster_data)\n",
    "            SR_HullInfoDict = Create_ConvexHullDict(SR_hull)\n",
    "        else:\n",
    "            SR_hull = None\n",
    "            SR_HullInfoDict = None\n",
    "\n",
    "        Tree_hull = ConvexHull(Full_TreePoints)\n",
    "\n",
    "        TreeClusterCentroid = np.mean(Full_TreePoints, axis=0)\n",
    "        latitude , longitude = ConvertLatLong(TreeClusterCentroid)\n",
    "        \n",
    "        Tree_HullInfoDict = Create_ConvexHullDict(Tree_hull)\n",
    "        \n",
    "        JSON_data_buffer = {\n",
    "            \"lasFileName\" : filename,\n",
    "            \"RecordedYear\" : year,\n",
    "            \"Tree_CountId\" : Global_TreeCounter,\n",
    "            \"MR_points\" : MR_Cluster_data.tolist(),\n",
    "            \"SR_points\" : SR_Cluster_data.tolist(),\n",
    "            \"Tree_Points\" : Full_TreePoints.tolist(),\n",
    "            \"PredictedTreeLocation\" : {\n",
    "                \"Latitude\" : latitude,\n",
    "                \"Longitude\" : longitude\n",
    "            },\n",
    "            \"TreeFoliageHeight\" : Tree_hull.points[:,2].max() - Tree_hull.points[:,2].min(),\n",
    "            \"GroundZValue\" : Elevation,\n",
    "            \"ClusterCentroid\" : TreeClusterCentroid.tolist(),\n",
    "            \"ConvexHull_MRDict\" : MR_HullInfoDict,\n",
    "            \"ConvexHull_SRDict\" : SR_HullInfoDict,\n",
    "            \"ConvexHull_TreeDict\" : Tree_HullInfoDict,\n",
    "            \"InPark\" : isPark\n",
    "        }\n",
    "        \n",
    "        jfilepath = str(las_fileID)+\"_\"+str(year)+\"_ID_\"+str(Global_TreeCounter)+\"_TreeCluster.json\"\n",
    "\n",
    "        # # Check whether the specified jfilepath exists or not\n",
    "        # isExist = os.path.exists(jfilepath)\n",
    "\n",
    "        # if not isExist:\n",
    "        # # Create a new directory because it does not exist \n",
    "        #     os.makedirs(jfilepath)\n",
    "\n",
    "        #Save JSON Buffer\n",
    "        with open(jfolderpath+\"/\"+jfilepath, \"w\") as jsonFile:\n",
    "            \n",
    "            jsonFile.truncate(0)\n",
    "            json.dump(JSON_data_buffer, jsonFile)\n",
    "        \n",
    "            logging.info(\"Tree JSON File Created - Tree ID : %d\",Global_TreeCounter)\n",
    "    \n",
    "    return Global_TreeCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGpoints_df = pd.DataFrame(NGpoints,columns=[\"X\",\"Y\",\"Z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONfoldernName = \"JSON_TreeData_\"+f[:-4]\n",
    "\n",
    "jfolderpath = \"Datasets/\" + \"Package_Generated/\"+f[:-4]+\"/\"+str(year)+\"/\" + JSONfoldernName +\"/\"\n",
    "\n",
    "# Check whether the specified jfolderpath exists or not\n",
    "isExist = os.path.exists(jfolderpath)\n",
    "\n",
    "if not isExist:\n",
    "# Create a new directory because it does not exist \n",
    "    os.makedirs(jfolderpath)\n",
    "\n",
    "# labels = [PC_labels,LC_labels,AC_labels]\n",
    "# point_arr = [Park_CLusterPoints, Large_ClustersPoints, Accepted_clustersPoints]\n",
    "# arr = Park_CLusterPoints\n",
    "Global_TreeCounter = 0\n",
    "\n",
    "\n",
    "las_fileID = f[:-4]\n",
    "\n",
    "Extracted_SRpoints = []\n",
    "Extracted_NTpoints = []\n",
    "\n",
    "\n",
    "Global_TreeCounter = Store_TreeCluster_toJSON(f, year,\n",
    "                            Park_CLusterPoints, PC_labels,\n",
    "                            NGpoints_df, \n",
    "                            Extracted_SRpoints, Extracted_NTpoints,\n",
    "                            Global_TreeCounter,\n",
    "                            Elevation,\n",
    "                            jfolderpath,isPark=True)\n",
    "\n",
    "Global_TreeCounter = Store_TreeCluster_toJSON(f, year,\n",
    "                            Large_ClustersPoints, LC_labels,\n",
    "                            NGpoints_df, \n",
    "                            Extracted_SRpoints, Extracted_NTpoints,\n",
    "                            Global_TreeCounter + 1,\n",
    "                            Elevation,\n",
    "                            jfolderpath)\n",
    "\n",
    "Global_TreeCounter = Store_TreeCluster_toJSON(f, year,\n",
    "                            Accepted_clustersPoints, AC_labels,\n",
    "                            NGpoints_df, \n",
    "                            Extracted_SRpoints, Extracted_NTpoints,\n",
    "                            Global_TreeCounter + 1,\n",
    "                            Elevation,\n",
    "                            jfolderpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new .las file with new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Classified_points = np.concatenate((Gpoints, Trees_Buffer,Extracted_SRpoints,Extracted_NTpoints), axis=0)\n",
    "\n",
    "class_1Labels = [1]*(len(Gpoints))\n",
    "class_2Labels = [2]*(len(Trees_Buffer))\n",
    "class_3Labels = [3]*(len(Extracted_SRpoints))\n",
    "class_4Labels = [4]*(len(Extracted_NTpoints))\n",
    "\n",
    "final_labels = np.concatenate((class_1Labels,class_2Labels,class_3Labels,class_4Labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiedpoints_df = pd.DataFrame(All_Classified_points,columns=[\"X\",\"Y\",\"Z\"])\n",
    "Classifiedpoints_df = Classifiedpoints_df.drop_duplicates()\n",
    "Original_lidarpoints_df = lidar_df.iloc[:,:3]\n",
    "# Unclassified_Points = Original_lidarpoints_df[~Classifiedpoints_df.isin(Original_lidarpoints_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = All_Classified_points\n",
    "arr2 = Original_lidarpoints_df.to_numpy()\n",
    "\n",
    "arr2_set = np.unique(Original_lidarpoints_df.to_numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=Original_lidarpoints_df\n",
    "df2=Classifiedpoints_df\n",
    "\n",
    "df_merged = df1.merge(df2, how=\"left\", on=['X','Y','Z'], indicator=True)\n",
    "Unclassified_Points_df =  df_merged.query(\"_merge == 'left_only'\")[['X','Y','Z']]\n",
    "\n",
    "Unclassified_Points = Unclassified_Points_df.to_numpy()\n",
    "# merged_df = pd.merge(df1, df2, how='left', on=['X','Y','Z'])\n",
    "# Unclassified_Points = merged_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1Labels = [1]*(len(Gpoints))\n",
    "class_2Labels = [2]*(len(Trees_Buffer))\n",
    "class_3Labels = [3]*(len(Extracted_SRpoints))\n",
    "class_4Labels = [4]*(len(Extracted_NTpoints))\n",
    "class_5Labels = [5]*(len(Unclassified_Points))\n",
    "\n",
    "final_labels = np.concatenate((class_1Labels,class_2Labels,class_3Labels,class_4Labels,class_5Labels),axis=0)\n",
    "All_points = np.concatenate((Gpoints, Trees_Buffer,Extracted_SRpoints,Extracted_NTpoints,Unclassified_Points), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clippedLasNumpy = All_points\n",
    "las = laspy.create(file_version=\"1.4\", point_format=3)\n",
    "\n",
    "Xscale = 0.01\n",
    "Yscale = 0.01\n",
    "Zscale = 0.01\n",
    "Xoffset = 0\n",
    "Yoffset = 0\n",
    "Zoffset = 0\n",
    "\n",
    "las.header.offsets = [Xoffset,Yoffset,Zoffset]\n",
    "las.header.scales = [Xscale,Yscale,Zscale]\n",
    "las.x = clippedLasNumpy[:, 0]\n",
    "las.y = clippedLasNumpy[:, 1]\n",
    "las.z = clippedLasNumpy[:, 2]\n",
    "las.intensity = [0]*len(clippedLasNumpy)\n",
    "las.classification =  final_labels\n",
    "las.return_number =  [0]*len(clippedLasNumpy)\n",
    "las.number_of_returns =  [0]*len(clippedLasNumpy)\n",
    "\n",
    "generated_lasfolderpath =  \"Datasets/\" + \"Package_Generated/\"+f[:-4]+\"/\"+str(year)+\"/LasClassified_\"+f[:-4]+\"/\"\n",
    "# Check whether the specified generated_lasfolderpath exists or not\n",
    "isExist = os.path.exists(generated_lasfolderpath)\n",
    "if not isExist:\n",
    "# Create a new directory because it does not exist \n",
    "    os.makedirs(generated_lasfolderpath)\n",
    "    \n",
    "generated_lasfilename = \"lasFile_Reconstructed_\"+f\n",
    "\n",
    "las.write(generated_lasfolderpath+generated_lasfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_lasfolderpath =  \"Datasets/\" + \"Package_Generated/\"+f[:-4]+\"/\"+str(year)+\"/LasClassified_\"+f[:-4]+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting new classified las file\n",
    "\n",
    "Gen_las = laspy.read(generated_lasfolderpath+generated_lasfilename)\n",
    "\n",
    "Xscale = Gen_las.header.x_scale\n",
    "Yscale = Gen_las.header.y_scale\n",
    "Zscale = Gen_las.header.z_scale\n",
    "\n",
    "Xoffset = Gen_las.header.x_offset\n",
    "Yoffset = Gen_las.header.y_offset\n",
    "Zoffset = Gen_las.header.z_offset\n",
    "\n",
    "Gen_lidarpoints = np.array(\n",
    "    ( (Gen_las.X*1.00) + Xoffset,  # convert ft to m and correct measurement\n",
    "      (Gen_las.Y*1.00) + Yoffset,\n",
    "      (Gen_las.Z*1.00) + Zoffset,\n",
    "    Gen_las.intensity,\n",
    "    Gen_las.classification,\n",
    "    Gen_las.return_number, \n",
    "    Gen_las.number_of_returns)).transpose()\n",
    "G_lidar_df = pd.DataFrame(Gen_lidarpoints , columns=['X','Y','Z','intensity','classification','return_number','number_of_returns'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_1Labels = [1]*(len(Gpoints))\n",
    "# class_2Labels = [2]*(len(Trees_Buffer))\n",
    "# class_3Labels = [3]*(len(Extracted_SRpoints))\n",
    "# class_4Labels = [4]*(len(Extracted_NTpoints))\n",
    "\n",
    "G_las_Gpoints = G_lidar_df.iloc[:,:3][G_lidar_df[\"classification\"] == 1].to_numpy()\n",
    "G_las_Tpoints = G_lidar_df.iloc[:,:3][G_lidar_df[\"classification\"] == 2].to_numpy()\n",
    "G_las_SRpoints = G_lidar_df.iloc[:,:3][G_lidar_df[\"classification\"] == 3].to_numpy()\n",
    "G_las_NTpoints = G_lidar_df.iloc[:,:3][G_lidar_df[\"classification\"] == 4].to_numpy()\n",
    "G_las_NCpoints = G_lidar_df.iloc[:,:3][G_lidar_df[\"classification\"] == 5].to_numpy()\n",
    "\n",
    "#plotting inlier and outlier\n",
    "All_points_1 = np.concatenate((G_las_Gpoints, G_las_Tpoints,G_las_SRpoints,G_las_NTpoints,G_las_NCpoints), axis=0)\n",
    "rgb_Ground =  [[1,0,0]]*len(G_las_Gpoints) #Set red colour\n",
    "rgb_Tree = [[0,1,0]]*len(G_las_Tpoints) #set green colour\n",
    "rgb_SR = [[0,0,1]]*len(G_las_SRpoints) #set blue colour\n",
    "rgb_NT = [[255,255,255]]*len(G_las_NTpoints) #set white colour\n",
    "rgb_NC = [[255,255,0]]*len(G_las_NCpoints) #set cyan colour\n",
    "All_rgb = np.concatenate((rgb_Ground, rgb_Tree,rgb_SR,rgb_NT,rgb_NC), axis=0)\n",
    "\n",
    "#Red - Inlier - ground plane , Green - Outlier\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)\n",
    "\n",
    "TakeScreenShot(All_points_1,v,f,year,\"Classified_LasFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etime = time.time()\n",
    "logging.info(\"Completed file: %s - Total Time Taken : \", f, etime - stime)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLotting from generated las file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading las file\n",
    "laspath = '/Users/sarangpramode/Desktop/Hub/TerraVide/Datasets/Package_Generated/25192/2017/LasClassified_25192/lasFile_Reconstructed_25192.las' #Datasets/Package_Generated/915120/2017/LasClassified_915120/lasFile_Reconstructed_915120.las'\n",
    "#laspath = '/Users/sarangpramode/Desktop/Hub/TerraVide/Datasets/Package_Generated/922135/2017/LasClassified_922135/lasFile_Reconstructed_922135.las'\n",
    "#Plotting new classified las file\n",
    "\n",
    "test_las = laspy.read(laspath)\n",
    "\n",
    "Xscale = test_las.header.x_scale\n",
    "Yscale = test_las.header.y_scale\n",
    "Zscale = test_las.header.z_scale\n",
    "\n",
    "Xoffset = test_las.header.x_offset\n",
    "Yoffset = test_las.header.y_offset\n",
    "Zoffset = test_las.header.z_offset\n",
    "\n",
    "test_lidarpoints = np.array(\n",
    "    ( (test_las.X*1.00) + Xoffset,  # convert ft to m and correct measurement\n",
    "      (test_las.Y*1.00) + Yoffset,\n",
    "      (test_las.Z*1.00) + Zoffset,\n",
    "    test_las.intensity,\n",
    "    test_las.classification,\n",
    "    test_las.return_number, \n",
    "    test_las.number_of_returns)).transpose()\n",
    "T_lidar_df = pd.DataFrame(test_lidarpoints , columns=['X','Y','Z','intensity','classification','return_number','number_of_returns'])\n",
    "\n",
    "# class_1Labels = [1]*(len(Gpoints))\n",
    "# class_2Labels = [2]*(len(Trees_Buffer))\n",
    "# class_3Labels = [3]*(len(Extracted_SRpoints))\n",
    "# class_4Labels = [4]*(len(Extracted_NTpoints))\n",
    "\n",
    "G_las_Gpoints = T_lidar_df.iloc[:,:3][T_lidar_df[\"classification\"] == 1].to_numpy()\n",
    "G_las_Tpoints = T_lidar_df.iloc[:,:3][T_lidar_df[\"classification\"] == 2].to_numpy()\n",
    "G_las_SRpoints = T_lidar_df.iloc[:,:3][T_lidar_df[\"classification\"] == 3].to_numpy()\n",
    "G_las_NTpoints = T_lidar_df.iloc[:,:3][T_lidar_df[\"classification\"] == 4].to_numpy()\n",
    "G_las_NCpoints = T_lidar_df.iloc[:,:3][T_lidar_df[\"classification\"] == 5].to_numpy()\n",
    "\n",
    "#plotting inlier and outlier\n",
    "#All_points_1 = np.concatenate((G_las_Gpoints, G_las_Tpoints,G_las_SRpoints,G_las_NTpoints,G_las_NCpoints), axis=0)\n",
    "All_points_1 = np.concatenate((G_las_Gpoints, G_las_Tpoints,G_las_SRpoints,G_las_NTpoints), axis=0)\n",
    "\n",
    "rgb_Ground =  [[1,0,0]]*len(G_las_Gpoints) #Set red colour\n",
    "rgb_Tree = [[0,1,0]]*len(G_las_Tpoints) #set green colour\n",
    "rgb_SR = [[0,0,1]]*len(G_las_SRpoints) #set blue colour\n",
    "rgb_NT = [[255,255,255]]*len(G_las_NTpoints) #set white colour\n",
    "rgb_NC = [[255,255,0]]*len(G_las_NCpoints) #set yellow colour\n",
    "#All_rgb = np.concatenate((rgb_Ground, rgb_Tree,rgb_SR,rgb_NT,rgb_NC), axis=0)\n",
    "All_rgb = np.concatenate((rgb_Ground, rgb_Tree,rgb_SR,rgb_NT), axis=0)\n",
    "\n",
    "#Red - Inlier - ground plane , Green - Outlier\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,1])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Park_CLusterPoints\n",
    "p2 = Extracted_SRpoints\n",
    "All_points_1 = np.concatenate((p1, p2), axis=0)\n",
    "rgb_p2 =  [[255,255,255]]*len(p2) #Set white colour - Not Ground\n",
    "rgb_p1 = [[1,0,0]]*len(p1) #set red colour - Ground\n",
    "All_rgb = np.concatenate((rgb_p1, rgb_p2), axis=0)\n",
    "\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,0])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array = assign_colors(Trees_Buffer, accepted_clusters)\n",
    "v = pptk.viewer(Trees_Buffer)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,0])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pptk.viewer(Filtered_TreeBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=np.mean(EPS_distribution_df.EPS), min_samples=30).fit(Trees_Buffer)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "DB_labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, count = np.unique(DB_labels,return_counts=True)\n",
    "label_count_arr = np.asarray([labels , count]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = 2000 #want to ignore clusters from with multiple trees\n",
    "filtered_cloud_labels = []\n",
    "retained_cloud_labels = []\n",
    "for i in label_count_arr:\n",
    "    \n",
    "    if i[1] > Threshold:\n",
    "        filtered_cloud_labels.append(i[0])\n",
    "    else:\n",
    "        retained_cloud_labels.append(i[0])\n",
    "retained_cloud_labels = np.array(retained_cloud_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_TB_cloud = []\n",
    "\n",
    "for fl in filtered_cloud_labels:\n",
    "    if fl == -1:\n",
    "        continue\n",
    "    indexes = np.where(fl == DB_labels)\n",
    "    points = Trees_Buffer[indexes]\n",
    "    for p in points:\n",
    "        filtered_TB_cloud.append(p)\n",
    "filtered_TB_cloud=np.array(filtered_TB_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_TB_cloud = []\n",
    "\n",
    "for rl in retained_cloud_labels:\n",
    "    if rl == -1:\n",
    "        continue\n",
    "    indexes = np.where(rl == DB_labels)\n",
    "    points = Trees_Buffer[indexes]\n",
    "    for p in points:\n",
    "        retained_TB_cloud.append(p)\n",
    "retained_TB_cloud=np.array(retained_TB_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pptk.viewer(retained_TB_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trees_Buffer.shape, LM_cluster_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGpoints_df['nearestNeighbor'] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of unique cluster labels\n",
    "clusters = NGpoints_df['nearestNeighbor']\n",
    "labels = set(clusters)\n",
    "# Create a dictionary to map cluster labels to colors\n",
    "color_map = {label: np.random.randint(0, 255, 3) for label in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pptk.viewer(NGpoints)\n",
    "v.attributes(rgb_array/255)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,0])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15,15), dpi=300, constrained_layout=True)\n",
    "# ax1 = fig.add_subplot(1, 1, 1, projection='3d') ############      \n",
    "# ax1.scatter3D(NGpoints_df['X'], NGpoints_df['Y'], NGpoints_df['Z'], zdir='z', s=0.11, c=NGpoints_df['nearestNeighbor'], marker='o', depthshade=True)\n",
    "# ax1.grid(False)\n",
    "# ax1.set_axis_off()           \n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting tree poitns found\n",
    "p1 = local_maximas\n",
    "p2 = NGpoints\n",
    "All_points_1 = np.concatenate((p1, p2), axis=0)\n",
    "rgb_p2 =  [[255,255,255]]*len(p2) #Set white colour - Not Ground\n",
    "rgb_p1 = [[1,0,0]]*len(p1) #set red colour - Ground\n",
    "All_rgb = np.concatenate((rgb_p1, rgb_p2), axis=0)\n",
    "\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,0])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Taking time - >10min(not completed) for 2964543 points\n",
    "maxSearch = df\n",
    "maxSearch['seeds'] = 1\n",
    "xs = maxSearch['X'].to_numpy()\n",
    "ys = maxSearch['Y'].to_numpy()\n",
    "zs = maxSearch['Z'].to_numpy()\n",
    "seeds = maxSearch['seeds'].to_numpy()\n",
    "\n",
    "neighborhood = 4\n",
    "changes = 1\n",
    "while changes > 0:\n",
    "    changes = 0\n",
    "    i = 0\n",
    "    for x,y,z,seed in zip(xs,ys,zs,seeds):\n",
    "        if seed == 1:\n",
    "            zsearch = zs[xs > x - neighborhood]\n",
    "            xsearch = xs[xs > x - neighborhood]\n",
    "            ysearch = ys[xs > x - neighborhood]\n",
    "            \n",
    "            zsearch = zsearch[xsearch < x + neighborhood]\n",
    "            ysearch = ysearch[xsearch < x + neighborhood]\n",
    "            \n",
    "            zsearch = zsearch[ysearch > y - neighborhood]\n",
    "            ysearch = ysearch[ysearch > y - neighborhood]\n",
    "            \n",
    "            zsearch = zsearch[ysearch < y + neighborhood]\n",
    "\n",
    "            zmax = np.max(zsearch)\n",
    "            if z < zmax:\n",
    "                seeds[i] = 0\n",
    "                changes += 1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        i+=1\n",
    "\n",
    "maxSearch['seeds'] = seeds      \n",
    "localMaxima = maxSearch[maxSearch['seeds']>0]\n",
    "\n",
    "#\n",
    "\n",
    "centers=localMaxima[['X','Y','Z']].to_numpy()\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(centers)\n",
    "points = maxSearch[['X','Y','Z']].to_numpy()\n",
    "distances, indices = nbrs.kneighbors(points)\n",
    "maxSearch['nearestNeighbor'] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "395b5501581aedb2d94c1f1944a406c3cb0aeb8faa5df16e5001f1dc5b0910fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
